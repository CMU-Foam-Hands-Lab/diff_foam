{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QwO2gAgiJS2","outputId":"eb446946-0d27-4753-b593-258fbae48d7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.10.12\n","Requirement already satisfied: torch==1.13.1 in ./.venv/lib/python3.10/site-packages (1.13.1)\n","Requirement already satisfied: torchvision==0.14.1 in ./.venv/lib/python3.10/site-packages (0.14.1)\n","Collecting diffusers==0.11.1\n","  Using cached diffusers-0.11.1-py3-none-any.whl (524 kB)\n","Requirement already satisfied: scikit-image==0.19.3 in ./.venv/lib/python3.10/site-packages (0.19.3)\n","Requirement already satisfied: scikit-video==1.1.11 in ./.venv/lib/python3.10/site-packages (1.1.11)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (8.5.0.96)\n","Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (4.12.2)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (11.10.3.66)\n","Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision==0.14.1) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision==0.14.1) (10.4.0)\n","Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from torchvision==0.14.1) (2.32.3)\n","Requirement already satisfied: huggingface-hub>=0.10.0 in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (0.24.5)\n","Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (3.15.4)\n","Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (2024.7.24)\n","Requirement already satisfied: importlib-metadata in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (8.2.0)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (24.1)\n","Requirement already satisfied: networkx>=2.2 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (3.3)\n","Requirement already satisfied: imageio>=2.4.1 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (2.34.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (2024.7.24)\n","Requirement already satisfied: PyWavelets>=1.1.1 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.6.0)\n","Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.14.0)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (65.5.0)\n","Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.44.0)\n","Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.66.5)\n","Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (6.0.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (2024.6.1)\n","Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.11.1) (3.19.2)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2024.7.4)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.3.2)\n","Installing collected packages: diffusers\n","  Attempting uninstall: diffusers\n","    Found existing installation: diffusers 0.29.2\n","    Uninstalling diffusers-0.29.2:\n","      Successfully uninstalled diffusers-0.29.2\n","Successfully installed diffusers-0.11.1\n","Requirement already satisfied: diffusers[torch] in ./.venv/lib/python3.10/site-packages (0.11.1)\n","Collecting diffusers[torch]\n","  Using cached diffusers-0.29.2-py3-none-any.whl (2.2 MB)\n","Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (1.26.4)\n","Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (3.15.4)\n","Requirement already satisfied: importlib-metadata in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (8.2.0)\n","Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (10.4.0)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (0.24.5)\n","Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (2.32.3)\n","Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (2024.7.24)\n","Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (0.4.4)\n","Requirement already satisfied: accelerate>=0.29.3 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (0.33.0)\n","Requirement already satisfied: torch>=1.4 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (1.13.1)\n","Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate>=0.29.3->diffusers[torch]) (6.0.0)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.29.3->diffusers[torch]) (24.1)\n","Requirement already satisfied: pyyaml in ./.venv/lib/python3.10/site-packages (from accelerate>=0.29.3->diffusers[torch]) (6.0.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (2024.6.1)\n","Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (4.12.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (11.10.3.66)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->diffusers[torch]) (65.5.0)\n","Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->diffusers[torch]) (0.44.0)\n","Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata->diffusers[torch]) (3.19.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (2.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (3.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (2024.7.4)\n","Installing collected packages: diffusers\n","  Attempting uninstall: diffusers\n","    Found existing installation: diffusers 0.11.1\n","    Uninstalling diffusers-0.11.1:\n","      Successfully uninstalled diffusers-0.11.1\n","Successfully installed diffusers-0.29.2\n","Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.17.0 in ./.venv/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n"]}],"source":["!python --version\n","!pip3 install setuptools==65.5.0 pip==22 > /dev/null\n","!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.11.1 scikit-image==0.19.3 scikit-video==1.1.11\n","!pip3 install --upgrade diffusers[torch]\n","!pip3 install opencv-python"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VrX4VTl5pYNq"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/foamlab/nw/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["CUDA is available: True\n","Number of GPUs: 1\n","CUDA version: 11.7\n","8500\n"]}],"source":["#### **Imports**\n","# diffusion policy import\n","from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n","import numpy as np\n","import math\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import collections\n","from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n","from diffusers.training_utils import EMAModel\n","from diffusers.optimization import get_scheduler\n","from tqdm.auto import tqdm\n","import torchvision.transforms as transforms\n","import pickle\n","# env import\n","import cv2\n","import os\n","\n","print(f\"CUDA is available: {torch.cuda.is_available()}\")\n","print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n","print(f\"CUDA version: {torch.version.cuda}\")\n","print(torch.backends.cudnn.version())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NK9ywrFUZedr"},"outputs":[],"source":["### data loading & saving\n","# tasks = [\"data-folder-name\"]\n","# task_name = \"task-name\"\n","tasks = [\"pkls\"]\n","task_name = \"tennis_sphere\"\n","data_path = \"rosbags/tennis_sphere/\"\n","save_path = \"save/\" + task_name + \".pt\"\n","dev = 'cuda'"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vHepJOFBucwg"},"outputs":[],"source":["### **Dataset**\n","# Defines `FoamDataset` and helper functions\n","# The dataset class\n","# Load data ((image, agent_pos), action) from pkl file\n","# Normalizes each dimension of agent_pos and action to [-1,1]\n","# Returns\n","# All possible segments with length `pred_horizon`\n","# Pads the beginning and the end of each episode with repetition\n","# key `image`: shape (obs_hoirzon, 3, 480, 640)\n","# key `agent_pos`: shape (obs_hoirzon, 23) # joint position for hand and arm\n","# key `action`: shape (pred_horizon, 23) # joint position for hand and arm\n","\n","def create_sample_indices(\n","        episode_ends:np.ndarray, sequence_length:int,\n","        pad_before: int=0, pad_after: int=0):\n","    indices = list()\n","    for i in range(len(episode_ends)):\n","        start_idx = 0\n","        if i > 0:\n","            start_idx = episode_ends[i-1]\n","        end_idx = episode_ends[i]\n","        episode_length = end_idx - start_idx\n","\n","        min_start = -pad_before\n","        max_start = episode_length - sequence_length + pad_after\n","\n","        # range stops one idx before end\n","        for idx in range(min_start, max_start+1):\n","            buffer_start_idx = max(idx, 0) + start_idx\n","            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n","            start_offset = buffer_start_idx - (idx+start_idx)\n","            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n","            sample_start_idx = 0 + start_offset\n","            sample_end_idx = sequence_length - end_offset\n","            indices.append([\n","                buffer_start_idx, buffer_end_idx,\n","                sample_start_idx, sample_end_idx])\n","    indices = np.array(indices)\n","    return indices\n","\n","\n","def sample_sequence(train_data, sequence_length,\n","                    buffer_start_idx, buffer_end_idx,\n","                    sample_start_idx, sample_end_idx):\n","    result = dict()\n","    for key, input_arr in train_data.items():\n","        sample = input_arr[buffer_start_idx:buffer_end_idx]\n","        data = sample\n","        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n","            data = np.zeros(\n","                shape=(sequence_length,) + input_arr.shape[1:],\n","                dtype=input_arr.dtype)\n","            if sample_start_idx > 0:\n","                data[:sample_start_idx] = sample[0]\n","            if sample_end_idx < sequence_length:\n","                data[sample_end_idx:] = sample[-1]\n","            data[sample_start_idx:sample_end_idx] = sample\n","        result[key] = data\n","    return result\n","\n","\n","def combine_data(cmd_hand, cmd_arm):\n","    assert cmd_hand.shape[0] == cmd_arm.shape[0], \"inconsistent number of samples\"\n","    combined_data = np.concatenate((cmd_hand, cmd_arm), axis=1)\n","    return combined_data\n","\n","# normalize data\n","def get_data_stats():\n","    min_values = np.array([0.000] * 23)  \n","    max_values = np.array([1.000] * 23) \n","    # Set specific min and max values for dimensions 4 and 12\n","    min_values[3] = -1.000  \n","    max_values[3] = 1.000   \n","    min_values[11] = -1.000  \n","    max_values[11] = 1.000   \n","    # Set specific min and max values for the last 7 dimensions (joint angles)\n","    min_values[16:] = np.array([-6.28, -2.06, -6.28, -0.19, -6.28, -1.69, -6.28])\n","    max_values[16:] = np.array([6.28, 2.09, 6.28, 3.93, 6.28, 3.14, 6.28])\n","    \n","    stats = {\n","        'min': min_values,\n","        'max': max_values\n","    }\n","    return stats\n","\n","def normalize_data(data, stats):\n","    ndata = np.copy(data)\n","    norm_range = stats['max'] - stats['min']\n","    indices_to_normalize = [i for i in range(23) if i not in [3, 11]]\n","    for i in indices_to_normalize:\n","        ndata[i] = (data[i] - stats['min'][i]) / norm_range[i] * 2 - 1\n","    return ndata\n","\n","def unnormalize_data(ndata, stats):\n","    data = np.copy(ndata)\n","    indices_to_normalize = [i for i in range(23) if i not in [3, 11]]\n","    for i in indices_to_normalize:\n","        ndata[i] = (ndata[i] + 1) / 2\n","    norm_range = stats['max'] - stats['min']\n","    for i in indices_to_normalize:\n","        data[i] = ndata[i] * norm_range[i] + stats['min'][i]\n","    return data\n","\n","def normalize_images(images):\n","    # resize image to (120, 160)\n","    # nomalize to [0,1]\n","    nimages = images / 255.0\n","    return nimages\n","\n","def add_noise(inputs):\n","     noise = torch.randn_like(inputs) * 0.2 - 0.1 #[-0.1, 0.1]\n","     return torch.clamp(inputs + noise, min=-1.0, max=1.0)\n","\n","transforms_noise = transforms.Compose([\n","    # transforms.RandomRotation(30),\n","    # transforms.RandomCrop(size=(216, 288)),\n","    transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5),\n","])\n","\n","# dataset\n","class FoamDataset(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 data_folder: list,\n","                 pred_horizon: int,\n","                 obs_horizon: int,\n","                 action_horizon: int):\n","\n","        # load all traj's data\n","        image_data = []\n","        actions = []\n","        states = []\n","        episode_ends = [] # the end idx of each traj\n","        cur_cnt = 0\n","        for folder in data_folder:\n","          data_list = sorted([data for data in os.listdir(folder) if data.endswith(\".pkl\")])\n","          for i in range(len(data_list)):\n","              with open(folder + data_list[i], 'rb') as f:\n","                data = pickle.load(f)\n","                image_data.append(np.array(data['image'])) # (480, 640, 3) # 0 - 255\n","\n","                cmd_hand = np.array(data['cmd_hand'])  # shape (N, 16)\n","                cmd_arm = np.array(data['cmd_xarm'])   # shape (N, 7)\n","                # print(cmd_hand.shape, cmd_arm.shape)\n","                combined_data = np.concatenate((cmd_hand, cmd_arm), axis=1)\n","                # print(combined_data.shape)\n","                cur_state = np.array(combined_data)\n","\n","                next_state = np.zeros_like(cur_state)\n","                next_state[:-1,:] = cur_state[1:,:]\n","                next_state[-1,:] = next_state[-2,:]\n","                action = next_state # predict next timestamp's pos as the action\n","\n","                states.append(cur_state)\n","                actions.append(action)\n","                cur_cnt += len(cur_state)\n","                episode_ends.append(cur_cnt)\n","\n","        print(\"Concatenating images...\")\n","        image_data = np.concatenate(image_data)\n","        episode_ends = np.array(episode_ends)\n","        print(\"Concatenating actions...\")\n","        actions = np.concatenate(actions)\n","        print(\"Concatenating states...\")\n","        states = np.concatenate(states)\n","\n","        # float32, [0,1], (N, 480, 640, 3) (N, 480, 640, 3)\n","        # print(\"Normalizing images...\")\n","        train_image_data = image_data\n","        print(\"Train image size: \", train_image_data.shape)\n","        print(\"Swaping image idex...\")\n","        train_image_data = np.moveaxis(train_image_data, -1,1) # (N, 3, 480, 640)\n","\n","        # (N, D)\n","        train_data = {\n","            # first two dims of state vector are agent (i.e. gripper) locations\n","            'agent_pos': states,\n","            'action': actions\n","        }\n","\n","        # compute start and end of each state-action sequence\n","        # also handles padding\n","        print(\"Creating sample indices...\")\n","        indices = create_sample_indices(\n","            episode_ends=episode_ends,\n","            sequence_length=pred_horizon,\n","            pad_before=obs_horizon-1,\n","            pad_after=action_horizon-1)\n","\n","        # compute statistics and normalized data to [-1,1]\n","        stats = dict()\n","        normalized_train_data = dict()\n","        for key, data in train_data.items():\n","            stats[key] = get_data_stats()\n","            normalized_train_data[key] = normalize_data(data, stats[key])\n","\n","        # images are already normalized\n","        normalized_train_data['image'] = train_image_data\n","\n","        self.indices = indices\n","        self.stats = stats\n","        self.normalized_train_data = normalized_train_data\n","        self.pred_horizon = pred_horizon\n","        self.action_horizon = action_horizon\n","        self.obs_horizon = obs_horizon\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        # get the start/end indices for this datapoint\n","        buffer_start_idx, buffer_end_idx, \\\n","            sample_start_idx, sample_end_idx = self.indices[idx]\n","\n","        # get nomralized data using these indices\n","        nsample = sample_sequence(\n","            train_data=self.normalized_train_data,\n","            sequence_length=self.pred_horizon,\n","            buffer_start_idx=buffer_start_idx,\n","            buffer_end_idx=buffer_end_idx,\n","            sample_start_idx=sample_start_idx,\n","            sample_end_idx=sample_end_idx\n","        )\n","\n","        # discard unused observations\n","        # we normalize images here!\n","        nsample['image'] = transforms_noise(torch.from_numpy(nsample['image'][:self.obs_horizon,:] / 255.0))\n","        # nsample['agent_pos'] = add_noise(torch.from_numpy(nsample['agent_pos'][:self.obs_horizon,:]))\n","        nsample['agent_pos'] = add_noise(torch.from_numpy(nsample['agent_pos'][:self.obs_horizon,:]).float())\n","\n","        nsample['action'] = torch.from_numpy(nsample['action'][:self.pred_horizon,:])\n","        return nsample\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9ZiHF3lzvB6k"},"outputs":[{"name":"stdout","output_type":"stream","text":["Concatenating images...\n","Concatenating actions...\n","Concatenating states...\n","Train image size:  (1737, 240, 320, 3)\n","Swaping image idex...\n","Creating sample indices...\n","{'agent_pos': {'min': array([ 0.  ,  0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n","        0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  , -6.28, -2.06,\n","       -6.28, -0.19, -6.28, -1.69, -6.28]), 'max': array([1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n","       1.  , 1.  , 1.  , 1.  , 1.  , 6.28, 2.09, 6.28, 3.93, 6.28, 3.14,\n","       6.28])}, 'action': {'min': array([ 0.  ,  0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n","        0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  , -6.28, -2.06,\n","       -6.28, -0.19, -6.28, -1.69, -6.28]), 'max': array([1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n","       1.  , 1.  , 1.  , 1.  , 1.  , 6.28, 2.09, 6.28, 3.93, 6.28, 3.14,\n","       6.28])}}\n","Creating dataloader...\n"]}],"source":["### load data\n","import os\n","import pickle\n","\n","data_folder = []\n","for task in tasks:\n","    data_folder.append(data_path + task + '/')\n","\n","# parameters\n","pred_horizon = 16\n","obs_horizon = 2\n","action_horizon = 8\n","#|o|o|                             observations: 2\n","#| |a|a|a|a|a|a|a|a|               actions executed: 8\n","#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n","\n","# create dataset from file\n","dataset = FoamDataset(\n","    data_folder=data_folder,\n","    pred_horizon=pred_horizon,\n","    obs_horizon=obs_horizon,\n","    action_horizon=action_horizon\n",")\n","# save training data statistics (min, max) for each dim\n","stats = dataset.stats\n","print(stats)\n","\n","# create dataloader\n","print(\"Creating dataloader...\")\n","dataloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=64,\n","    num_workers=2,\n","    shuffle=True,\n","    # accelerate cpu-gpu transfer\n","    pin_memory=False,\n","    # don't kill worker process afte each epoch\n","    persistent_workers=False\n",")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"X-XRB_g3vsgf"},"outputs":[],"source":["#@markdown ### **Network**\n","#@markdown\n","#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n","#@markdown as the noies prediction network\n","#@markdown\n","#@markdown Components\n","#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n","#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n","#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n","#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n","#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n","#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n","#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n","\n","class SinusoidalPosEmb(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        device = x.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = x[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","        return emb\n","\n","\n","class Downsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Upsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Conv1dBlock(nn.Module):\n","    '''\n","        Conv1d --> GroupNorm --> Mish\n","    '''\n","\n","    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n","        super().__init__()\n","\n","        self.block = nn.Sequential(\n","            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n","            nn.GroupNorm(n_groups, out_channels),\n","            nn.Mish(),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class ConditionalResidualBlock1D(nn.Module):\n","    def __init__(self,\n","            in_channels,\n","            out_channels,\n","            cond_dim,\n","            kernel_size=3,\n","            n_groups=8):\n","        super().__init__()\n","\n","        self.blocks = nn.ModuleList([\n","            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n","            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n","        ])\n","\n","        # FiLM modulation https://arxiv.org/abs/1709.07871\n","        # predicts per-channel scale and bias\n","        cond_channels = out_channels * 2\n","        self.out_channels = out_channels\n","        self.cond_encoder = nn.Sequential(\n","            nn.Mish(),\n","            nn.Linear(cond_dim, cond_channels),\n","            nn.Unflatten(-1, (-1, 1))\n","        )\n","\n","        # make sure dimensions compatible\n","        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n","            if in_channels != out_channels else nn.Identity()\n","\n","    def forward(self, x, cond):\n","        '''\n","            x : [ batch_size x in_channels x horizon ]\n","            cond : [ batch_size x cond_dim]\n","\n","            returns:\n","            out : [ batch_size x out_channels x horizon ]\n","        '''\n","        out = self.blocks[0](x)\n","        embed = self.cond_encoder(cond)\n","\n","        embed = embed.reshape(\n","            embed.shape[0], 2, self.out_channels, 1)\n","        scale = embed[:,0,...]\n","        bias = embed[:,1,...]\n","        out = scale * out + bias\n","\n","        out = self.blocks[1](out)\n","        out = out + self.residual_conv(x)\n","        return out\n","\n","\n","class ConditionalUnet1D(nn.Module):\n","    def __init__(self,\n","        input_dim,\n","        global_cond_dim,\n","        diffusion_step_embed_dim=256,\n","        down_dims=[256,512,1024],\n","        kernel_size=5,\n","        n_groups=8\n","        ):\n","        \"\"\"\n","        input_dim: Dim of actions.\n","        global_cond_dim: Dim of global conditioning applied with FiLM\n","          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n","        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n","        down_dims: Channel size for each UNet level.\n","          The length of this array determines numebr of levels.\n","        kernel_size: Conv kernel size\n","        n_groups: Number of groups for GroupNorm\n","        \"\"\"\n","\n","        super().__init__()\n","        all_dims = [input_dim] + list(down_dims)\n","        start_dim = down_dims[0]\n","\n","        dsed = diffusion_step_embed_dim\n","        diffusion_step_encoder = nn.Sequential(\n","            SinusoidalPosEmb(dsed),\n","            nn.Linear(dsed, dsed * 4),\n","            nn.Mish(),\n","            nn.Linear(dsed * 4, dsed),\n","        )\n","        cond_dim = dsed + global_cond_dim\n","\n","        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n","        mid_dim = all_dims[-1]\n","        self.mid_modules = nn.ModuleList([\n","            ConditionalResidualBlock1D(\n","                mid_dim, mid_dim, cond_dim=cond_dim,\n","                kernel_size=kernel_size, n_groups=n_groups\n","            ),\n","            ConditionalResidualBlock1D(\n","                mid_dim, mid_dim, cond_dim=cond_dim,\n","                kernel_size=kernel_size, n_groups=n_groups\n","            ),\n","        ])\n","\n","        down_modules = nn.ModuleList([])\n","        for ind, (dim_in, dim_out) in enumerate(in_out):\n","            is_last = ind >= (len(in_out) - 1)\n","            down_modules.append(nn.ModuleList([\n","                ConditionalResidualBlock1D(\n","                    dim_in, dim_out, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                ConditionalResidualBlock1D(\n","                    dim_out, dim_out, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                Downsample1d(dim_out) if not is_last else nn.Identity()\n","            ]))\n","\n","        up_modules = nn.ModuleList([])\n","        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n","            is_last = ind >= (len(in_out) - 1)\n","            up_modules.append(nn.ModuleList([\n","                ConditionalResidualBlock1D(\n","                    dim_out*2, dim_in, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                ConditionalResidualBlock1D(\n","                    dim_in, dim_in, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                Upsample1d(dim_in) if not is_last else nn.Identity()\n","            ]))\n","\n","        final_conv = nn.Sequential(\n","            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n","            nn.Conv1d(start_dim, input_dim, 1),\n","        )\n","\n","        self.diffusion_step_encoder = diffusion_step_encoder\n","        self.up_modules = up_modules\n","        self.down_modules = down_modules\n","        self.final_conv = final_conv\n","\n","        print(\"number of parameters: {:e}\".format(\n","            sum(p.numel() for p in self.parameters()))\n","        )\n","\n","    def forward(self,\n","            sample: torch.Tensor,\n","            timestep: Union[torch.Tensor, float, int],\n","            global_cond=None):\n","        \"\"\"\n","        x: (B,T,input_dim)\n","        timestep: (B,) or int, diffusion step\n","        global_cond: (B,global_cond_dim)\n","        output: (B,T,input_dim)\n","        \"\"\"\n","        # (B,T,C)\n","        sample = sample.moveaxis(-1,-2)\n","        # (B,C,T)\n","\n","        # 1. time\n","        timesteps = timestep\n","        if not torch.is_tensor(timesteps):\n","            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n","            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n","        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n","            timesteps = timesteps[None].to(sample.device)\n","        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n","        timesteps = timesteps.expand(sample.shape[0])\n","\n","        global_feature = self.diffusion_step_encoder(timesteps)\n","\n","        if global_cond is not None:\n","            global_feature = torch.cat([\n","                global_feature, global_cond\n","            ], axis=-1)\n","\n","        x = sample\n","        h = []\n","        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n","            x = resnet(x, global_feature)\n","            x = resnet2(x, global_feature)\n","            h.append(x)\n","            x = downsample(x)\n","\n","        for mid_module in self.mid_modules:\n","            x = mid_module(x, global_feature)\n","\n","        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n","            x = torch.cat((x, h.pop()), dim=1)\n","            x = resnet(x, global_feature)\n","            x = resnet2(x, global_feature)\n","            x = upsample(x)\n","\n","        x = self.final_conv(x)\n","\n","        # (B,C,T)\n","        x = x.moveaxis(-1,-2)\n","        # (B,T,C)\n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yXq4r744aMh1"},"outputs":[],"source":["#### **Vision Encoder**\n","# Defines helper functions:\n","# `get_resnet` to initialize standard ResNet vision encoder\n","# `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n","\n","def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n","    \"\"\"\n","    name: resnet18, resnet34, resnet50\n","    weights: \"IMAGENET1K_V1\", None\n","    \"\"\"\n","    # Use standard ResNet implementation from torchvision\n","    func = getattr(torchvision.models, name)\n","    resnet = func(weights=weights, **kwargs)\n","\n","    # remove the final fully connected layer\n","    # for resnet18, the output dim should be 512\n","    resnet.fc = torch.nn.Identity()\n","    return resnet\n","\n","\n","def replace_submodules(\n","        root_module: nn.Module,\n","        predicate: Callable[[nn.Module], bool],\n","        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n","    \"\"\"\n","    Replace all submodules selected by the predicate with\n","    the output of func.\n","\n","    predicate: Return true if the module is to be replaced.\n","    func: Return new module to use.\n","    \"\"\"\n","    if predicate(root_module):\n","        return func(root_module)\n","\n","    bn_list = [k.split('.') for k, m\n","        in root_module.named_modules(remove_duplicate=True)\n","        if predicate(m)]\n","    for *parent, k in bn_list:\n","        parent_module = root_module\n","        if len(parent) > 0:\n","            parent_module = root_module.get_submodule('.'.join(parent))\n","        if isinstance(parent_module, nn.Sequential):\n","            src_module = parent_module[int(k)]\n","        else:\n","            src_module = getattr(parent_module, k)\n","        tgt_module = func(src_module)\n","        if isinstance(parent_module, nn.Sequential):\n","            parent_module[int(k)] = tgt_module\n","        else:\n","            setattr(parent_module, k, tgt_module)\n","    # verify that all modules are replaced\n","    bn_list = [k.split('.') for k, m\n","        in root_module.named_modules(remove_duplicate=True)\n","        if predicate(m)]\n","    assert len(bn_list) == 0\n","    return root_module\n","\n","def replace_bn_with_gn(\n","    root_module: nn.Module,\n","    features_per_group: int=16) -> nn.Module:\n","    \"\"\"\n","    Relace all BatchNorm layers with GroupNorm.\n","    \"\"\"\n","    replace_submodules(\n","        root_module=root_module,\n","        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n","        func=lambda x: nn.GroupNorm(\n","            num_groups=x.num_features//features_per_group,\n","            num_channels=x.num_features)\n","    )\n","    return root_module\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4APZkqh336-M","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["number of parameters: 8.058703e+07\n","torch.Size([1, 2, 535])\n"]}],"source":["#### **Network Demo**\n","\n","# construct ResNet18 encoder\n","# if you have multiple camera views, use seperate encoder weights for each view.\n","vision_encoder = get_resnet('resnet18')\n","\n","# IMPORTANT!\n","# replace all BatchNorm with GroupNorm to work with EMA\n","# performance will tank if you forget to do this!\n","vision_encoder = replace_bn_with_gn(vision_encoder)\n","\n","# ResNet18 has output dim of 512\n","vision_feature_dim = 512\n","# agent_pos is 32 dimensional\n","lowdim_obs_dim = 23\n","# observation feature has 512+32 dims in total per step\n","obs_dim = vision_feature_dim + lowdim_obs_dim\n","action_dim = 23\n","\n","# create network object\n","noise_pred_net = ConditionalUnet1D(\n","    input_dim=action_dim,\n","    global_cond_dim=obs_dim*obs_horizon\n",")\n","\n","# the final arch has 2 parts\n","nets = nn.ModuleDict({\n","    'vision_encoder': vision_encoder,\n","    'noise_pred_net': noise_pred_net\n","})\n","\n","# demo\n","with torch.no_grad():\n","    # example inputs\n","    image = torch.zeros((1, obs_horizon,3,480,640))\n","    agent_pos = torch.zeros((1, obs_horizon, lowdim_obs_dim))\n","    # vision encoder\n","    image_features = nets['vision_encoder'](\n","        image.flatten(end_dim=1))\n","    # (2,512)\n","    image_features = image_features.reshape(*image.shape[:2],-1)\n","    # (1,2,512)\n","    obs = torch.cat([image_features, agent_pos],dim=-1)\n","    # (1,2,512+7)\n","    print(obs.shape)\n","\n","    noised_action = torch.randn((1, pred_horizon, action_dim))\n","    diffusion_iter = torch.zeros((1,))\n","\n","    # the noise prediction network\n","    # takes noisy action, diffusion iteration and observation as input\n","    # predicts the noise added to action\n","    noise = nets['noise_pred_net'](\n","        sample=noised_action,\n","        timestep=diffusion_iter,\n","        global_cond=obs.flatten(start_dim=1))\n","\n","    # illustration of removing noise\n","    # the actual noise removal is performed by NoiseScheduler\n","    # and is dependent on the diffusion noise schedule\n","    denoised_action = noised_action - noise\n","\n","# for this demo, we use DDPMScheduler with 100 diffusion iterations\n","num_diffusion_iters = 100\n","noise_scheduler = DDPMScheduler(\n","    num_train_timesteps=num_diffusion_iters,\n","    # the choise of beta schedule has big impact on performance\n","    # we found squared cosine works the best\n","    beta_schedule='squaredcos_cap_v2',\n","    # clip output to [-1,1] to improve stability\n","    clip_sample=True,\n","    # our network predicts noise (instead of denoised action)\n","    prediction_type='epsilon'\n",")\n","\n","# device transfer\n","device = torch.device(dev) #'cuda' # error on nvidia version\n","_ = nets.to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"93E9RdnR4D8v"},"outputs":[{"name":"stdout","output_type":"stream","text":["Skipped pretrained weight loading.\n"]},{"name":"stderr","output_type":"stream","text":["/home/foamlab/nw/.venv/lib/python3.10/site-packages/diffusers/training_utils.py:361: FutureWarning: Passing a `torch.nn.Module` to `ExponentialMovingAverage.step` is deprecated. Please pass the parameters of the module instead.\n","  deprecate(\n"]},{"name":"stdout","output_type":"stream","text":["# epoch, loss:  0 1.0432189658836082\n","A checkpoint is saved at # epoch!  0\n","# epoch, loss:  1 0.9978888498412238\n","A checkpoint is saved at # epoch!  1\n","# epoch, loss:  2 0.9807395316936351\n","A checkpoint is saved at # epoch!  2\n","# epoch, loss:  3 0.890034141363921\n","A checkpoint is saved at # epoch!  3\n","# epoch, loss:  4 0.7673270503679911\n","A checkpoint is saved at # epoch!  4\n","# epoch, loss:  5 0.6638060145907931\n","A checkpoint is saved at # epoch!  5\n","# epoch, loss:  6 0.5849333957389549\n","A checkpoint is saved at # epoch!  6\n","# epoch, loss:  7 0.507172754517308\n","A checkpoint is saved at # epoch!  7\n","# epoch, loss:  8 0.44793590792903193\n","A checkpoint is saved at # epoch!  8\n","# epoch, loss:  9 0.3901681900024414\n","A checkpoint is saved at # epoch!  9\n","# epoch, loss:  10 0.3380724975356349\n","A checkpoint is saved at # epoch!  10\n","# epoch, loss:  11 0.29332312168898406\n","A checkpoint is saved at # epoch!  11\n","# epoch, loss:  12 0.26585396848343035\n","A checkpoint is saved at # epoch!  12\n","# epoch, loss:  13 0.23294022624139432\n","A checkpoint is saved at # epoch!  13\n","# epoch, loss:  14 0.20566739362699013\n","A checkpoint is saved at # epoch!  14\n","# epoch, loss:  15 0.1829349751825686\n","A checkpoint is saved at # epoch!  15\n","# epoch, loss:  16 0.1682096940499765\n","A checkpoint is saved at # epoch!  16\n","# epoch, loss:  17 0.1551314702740422\n","A checkpoint is saved at # epoch!  17\n","# epoch, loss:  18 0.13517673313617706\n","A checkpoint is saved at # epoch!  18\n","# epoch, loss:  19 0.12660194050382684\n","A checkpoint is saved at # epoch!  19\n","# epoch, loss:  20 0.11787980491364444\n","A checkpoint is saved at # epoch!  20\n","# epoch, loss:  21 0.1078742543856303\n","A checkpoint is saved at # epoch!  21\n","# epoch, loss:  22 0.10191269080947947\n","A checkpoint is saved at # epoch!  22\n","# epoch, loss:  23 0.09828342083427641\n","A checkpoint is saved at # epoch!  23\n","# epoch, loss:  24 0.09198954177123529\n","A checkpoint is saved at # epoch!  24\n","# epoch, loss:  25 0.08622958524911492\n","A checkpoint is saved at # epoch!  25\n","# epoch, loss:  26 0.08467378439726653\n","A checkpoint is saved at # epoch!  26\n","# epoch, loss:  27 0.07924824773713394\n","A checkpoint is saved at # epoch!  27\n","# epoch, loss:  28 0.08246076024240917\n","# epoch, loss:  29 0.0721892290921123\n","A checkpoint is saved at # epoch!  29\n","# epoch, loss:  30 0.07456877179167888\n","# epoch, loss:  31 0.0754918730645268\n","# epoch, loss:  32 0.07029604373706712\n","A checkpoint is saved at # epoch!  32\n","# epoch, loss:  33 0.06253491714596748\n","A checkpoint is saved at # epoch!  33\n","# epoch, loss:  34 0.06110254633757803\n","A checkpoint is saved at # epoch!  34\n","# epoch, loss:  35 0.060027739929932135\n","A checkpoint is saved at # epoch!  35\n","# epoch, loss:  36 0.06048533554982256\n","# epoch, loss:  37 0.06394540270169576\n","# epoch, loss:  38 0.05626422967071886\n","A checkpoint is saved at # epoch!  38\n","# epoch, loss:  39 0.05586842199166616\n","A checkpoint is saved at # epoch!  39\n","# epoch, loss:  40 0.05241726449242345\n","A checkpoint is saved at # epoch!  40\n","# epoch, loss:  41 0.05165799679579558\n","A checkpoint is saved at # epoch!  41\n","# epoch, loss:  42 0.04809145739784947\n","A checkpoint is saved at # epoch!  42\n","# epoch, loss:  43 0.04863358220016515\n","# epoch, loss:  44 0.05037720418638653\n","# epoch, loss:  45 0.05101233789766276\n","# epoch, loss:  46 0.05244704156562134\n","# epoch, loss:  47 0.04743905324074957\n","A checkpoint is saved at # epoch!  47\n","# epoch, loss:  48 0.04907538755624383\n","# epoch, loss:  49 0.04579167951036383\n","A checkpoint is saved at # epoch!  49\n","# epoch, loss:  50 0.045559763770412515\n","A checkpoint is saved at # epoch!  50\n","# epoch, loss:  51 0.043278214142278386\n","A checkpoint is saved at # epoch!  51\n","# epoch, loss:  52 0.04462103180035397\n","# epoch, loss:  53 0.041566479468235266\n","A checkpoint is saved at # epoch!  53\n","# epoch, loss:  54 0.04789978658987416\n","# epoch, loss:  55 0.04289180109346354\n","# epoch, loss:  56 0.041575236866871514\n","# epoch, loss:  57 0.03972257411590329\n","A checkpoint is saved at # epoch!  57\n","# epoch, loss:  58 0.03990257821149296\n","# epoch, loss:  59 0.04016066915183156\n","# epoch, loss:  60 0.041130619744459786\n","# epoch, loss:  61 0.04000833540878914\n","# epoch, loss:  62 0.03886098752695101\n","A checkpoint is saved at # epoch!  62\n","# epoch, loss:  63 0.037366095171482476\n","A checkpoint is saved at # epoch!  63\n","# epoch, loss:  64 0.03962157880542455\n","# epoch, loss:  65 0.03869246415517948\n","# epoch, loss:  66 0.035978826245775926\n","A checkpoint is saved at # epoch!  66\n","# epoch, loss:  67 0.03428138240619942\n","A checkpoint is saved at # epoch!  67\n","# epoch, loss:  68 0.03640035601953665\n","# epoch, loss:  69 0.03492338403507515\n","# epoch, loss:  70 0.033910452698667846\n","A checkpoint is saved at # epoch!  70\n","# epoch, loss:  71 0.03368820698448905\n","A checkpoint is saved at # epoch!  71\n","# epoch, loss:  72 0.03577227987073086\n","# epoch, loss:  73 0.03240066649461234\n","A checkpoint is saved at # epoch!  73\n","# epoch, loss:  74 0.03186642426859449\n","A checkpoint is saved at # epoch!  74\n","# epoch, loss:  75 0.03346247363973547\n","# epoch, loss:  76 0.03346220645363684\n","# epoch, loss:  77 0.03138935724618258\n","A checkpoint is saved at # epoch!  77\n","# epoch, loss:  78 0.03256733529269695\n","# epoch, loss:  79 0.033690717416229074\n","# epoch, loss:  80 0.029286129952028946\n","A checkpoint is saved at # epoch!  80\n","# epoch, loss:  81 0.029889210851656065\n","# epoch, loss:  82 0.029680078297301574\n","# epoch, loss:  83 0.030956404697563913\n","# epoch, loss:  84 0.027442985662707576\n","A checkpoint is saved at # epoch!  84\n","# epoch, loss:  85 0.02972639303792406\n","# epoch, loss:  86 0.027363371931844287\n","A checkpoint is saved at # epoch!  86\n","# epoch, loss:  87 0.028144408776252357\n","# epoch, loss:  88 0.02916001549197568\n","# epoch, loss:  89 0.02778929568551205\n","# epoch, loss:  90 0.028191063967016008\n","# epoch, loss:  91 0.02662518379037027\n","A checkpoint is saved at # epoch!  91\n","# epoch, loss:  92 0.026314973417255614\n","A checkpoint is saved at # epoch!  92\n","# epoch, loss:  93 0.026751840280161962\n","# epoch, loss:  94 0.02735351315802998\n","# epoch, loss:  95 0.026184423309233453\n","A checkpoint is saved at # epoch!  95\n","# epoch, loss:  96 0.028305874309606023\n","# epoch, loss:  97 0.028973586167450303\n","# epoch, loss:  98 0.024012698619453994\n","A checkpoint is saved at # epoch!  98\n","# epoch, loss:  99 0.025078651184837025\n","# epoch, loss:  100 0.028320051867652823\n","# epoch, loss:  101 0.026274118848420954\n","# epoch, loss:  102 0.02718587054146661\n","# epoch, loss:  103 0.02572402454636715\n","# epoch, loss:  104 0.02755516712312345\n","# epoch, loss:  105 0.02538062383731206\n","# epoch, loss:  106 0.024301710159138398\n","# epoch, loss:  107 0.024305281608744903\n","# epoch, loss:  108 0.025054055507536286\n","# epoch, loss:  109 0.024487063150714944\n","# epoch, loss:  110 0.025054407030068063\n","# epoch, loss:  111 0.025295385263032384\n","# epoch, loss:  112 0.023315392365610157\n","A checkpoint is saved at # epoch!  112\n","# epoch, loss:  113 0.02311301189992163\n","A checkpoint is saved at # epoch!  113\n","# epoch, loss:  114 0.02374799987646165\n","# epoch, loss:  115 0.02384080875802923\n","# epoch, loss:  116 0.022912156285235175\n","A checkpoint is saved at # epoch!  116\n","# epoch, loss:  117 0.023171217974137376\n","# epoch, loss:  118 0.022669830669959385\n","A checkpoint is saved at # epoch!  118\n","# epoch, loss:  119 0.021660466850907716\n","A checkpoint is saved at # epoch!  119\n","# epoch, loss:  120 0.022174853417608473\n","# epoch, loss:  121 0.021291390406312765\n","A checkpoint is saved at # epoch!  121\n","# epoch, loss:  122 0.021383180283010006\n","# epoch, loss:  123 0.01878529273111511\n","A checkpoint is saved at # epoch!  123\n","# epoch, loss:  124 0.020658542919490073\n","# epoch, loss:  125 0.021009083250882448\n","# epoch, loss:  126 0.020973437120793043\n","# epoch, loss:  127 0.019487169502234017\n","# epoch, loss:  128 0.021287356054893247\n","# epoch, loss:  129 0.02160767652094364\n","# epoch, loss:  130 0.020981376197327067\n","# epoch, loss:  131 0.020978899914081448\n","# epoch, loss:  132 0.019649068411025736\n","# epoch, loss:  133 0.02076277198890845\n","# epoch, loss:  134 0.02046673796657059\n","# epoch, loss:  135 0.01988468119115741\n","# epoch, loss:  136 0.01992668646077315\n","# epoch, loss:  137 0.01911067258980539\n","# epoch, loss:  138 0.019871622186016152\n","# epoch, loss:  139 0.017909705949326355\n","A checkpoint is saved at # epoch!  139\n","# epoch, loss:  140 0.018185607778529327\n","# epoch, loss:  141 0.01783160585910082\n","A checkpoint is saved at # epoch!  141\n","# epoch, loss:  142 0.017729958657313277\n","A checkpoint is saved at # epoch!  142\n","# epoch, loss:  143 0.017953233551923877\n","# epoch, loss:  144 0.016833352614884025\n","A checkpoint is saved at # epoch!  144\n","# epoch, loss:  145 0.01885640721216246\n","# epoch, loss:  146 0.018370297426978748\n","# epoch, loss:  147 0.017455584955988108\n","# epoch, loss:  148 0.0170525000948045\n","# epoch, loss:  149 0.01709753868204576\n","# epoch, loss:  150 0.016721054946106893\n","A checkpoint is saved at # epoch!  150\n","# epoch, loss:  151 0.016239861233366862\n","A checkpoint is saved at # epoch!  151\n","# epoch, loss:  152 0.01727767233495359\n","# epoch, loss:  153 0.015925944472352665\n","A checkpoint is saved at # epoch!  153\n","# epoch, loss:  154 0.01644381635856849\n","# epoch, loss:  155 0.015648326802032965\n","A checkpoint is saved at # epoch!  155\n","# epoch, loss:  156 0.016102746818904525\n","# epoch, loss:  157 0.016119961488854\n","# epoch, loss:  158 0.015468740801292437\n","A checkpoint is saved at # epoch!  158\n","# epoch, loss:  159 0.015888948165984067\n","# epoch, loss:  160 0.016131540105022765\n","# epoch, loss:  161 0.014586347569194105\n","A checkpoint is saved at # epoch!  161\n","# epoch, loss:  162 0.012962125862638155\n","A checkpoint is saved at # epoch!  162\n","# epoch, loss:  163 0.015150117204972991\n","# epoch, loss:  164 0.015830132523896517\n","# epoch, loss:  165 0.014802955470427318\n","# epoch, loss:  166 0.015402007399609795\n","# epoch, loss:  167 0.01604505204078224\n","# epoch, loss:  168 0.015055457750956217\n","# epoch, loss:  169 0.015920215941689634\n","# epoch, loss:  170 0.014302449266391772\n","# epoch, loss:  171 0.014898613785152082\n","# epoch, loss:  172 0.014809445615995813\n","# epoch, loss:  173 0.013454908091160987\n","# epoch, loss:  174 0.013912641684766169\n","# epoch, loss:  175 0.01473042313699369\n","# epoch, loss:  176 0.013883031421789416\n","# epoch, loss:  177 0.013875795873226944\n","# epoch, loss:  178 0.012976416938558773\n","# epoch, loss:  179 0.014391082718416496\n","# epoch, loss:  180 0.015498806481008176\n","# epoch, loss:  181 0.013867743175338816\n","# epoch, loss:  182 0.014398356899619102\n","# epoch, loss:  183 0.01344274470789565\n","# epoch, loss:  184 0.01355265829436205\n","# epoch, loss:  185 0.013010792623929403\n","# epoch, loss:  186 0.013632077965195532\n","# epoch, loss:  187 0.012961443962046394\n","A checkpoint is saved at # epoch!  187\n","# epoch, loss:  188 0.013263525441288948\n","# epoch, loss:  189 0.013716506219848438\n","# epoch, loss:  190 0.013891842802641568\n","# epoch, loss:  191 0.013281357992026541\n","# epoch, loss:  192 0.01359212919379826\n","# epoch, loss:  193 0.013910299580958154\n","# epoch, loss:  194 0.013213151738185573\n","# epoch, loss:  195 0.014191569656961493\n","# epoch, loss:  196 0.012738244952978912\n","A checkpoint is saved at # epoch!  196\n","# epoch, loss:  197 0.012799531393856914\n","# epoch, loss:  198 0.012749390331683335\n","# epoch, loss:  199 0.013635764208932718\n"]}],"source":["#@markdown ### **Training**\n","#@markdown\n","#@markdown Takes about 2.5 hours. If you don't want to wait, skip to the next cell\n","#@markdown to load pre-trained weights\n","\n","num_epochs = 200\n","\n","# Exponential Moving Average\n","# accelerates training and improves stability\n","# holds a copy of the model weights\n","ema = EMAModel(\n","    parameters=nets.parameters(),\n","    power=0.75)\n","\n","# Standard ADAM optimizer\n","# Note that EMA parametesr are not optimized\n","optimizer = torch.optim.AdamW(\n","    params=nets.parameters(),\n","    lr=1e-4, weight_decay=1e-6)\n","\n","# Cosine LR schedule with linear warmup\n","lr_scheduler = get_scheduler(\n","    name='cosine',\n","    optimizer=optimizer,\n","    num_warmup_steps=500,\n","    num_training_steps=len(dataloader) * num_epochs\n",")\n","\n","cur_loss = 0.0\n","min_loss = np.inf\n","\n","load_pretrained = False\n","if load_pretrained:\n","  if not os.path.isfile(save_path):\n","      print(\"Err at loading trained model!\")\n","\n","  state_dict = torch.load(save_path, map_location='cuda')\n","  ema.load_state_dict(state_dict['model_state_dict'])\n","  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n","  print('Pretrained weights loaded.')\n","else:\n","  print(\"Skipped pretrained weight loading.\")\n","\n","# epoch loop\n","for epoch_idx in range(num_epochs):\n","    epoch_loss = list()\n","    # batch loop\n","    for nbatch in dataloader:\n","        # data normalized in dataset\n","        # device transfer\n","        nimage = nbatch['image'][:,:obs_horizon].to(device, dtype=torch.float)\n","        nagent_pos = nbatch['agent_pos'][:,:obs_horizon].to(device)\n","        naction = nbatch['action'].to(device)\n","        B = nagent_pos.shape[0]\n","\n","        # encoder vision features\n","        image_features = nets['vision_encoder'](\n","            nimage.flatten(end_dim=1))\n","        image_features = image_features.reshape(\n","            *nimage.shape[:2],-1)\n","        # (B,obs_horizon,D)\n","\n","        # concatenate vision feature and low-dim obs\n","        obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n","        obs_cond = obs_features.flatten(start_dim=1)\n","        # (B, obs_horizon * obs_dim)\n","\n","        # sample noise to add to actions\n","        noise = torch.randn(naction.shape, device=device, dtype=torch.float)\n","\n","        # sample a diffusion iteration for each data point\n","        timesteps = torch.randint(\n","            0, noise_scheduler.config.num_train_timesteps,\n","            (B,), device=device\n","        ).long()\n","\n","\n","        # add noise to the clean images according to the noise magnitude at each diffusion iteration\n","        # (this is the forward diffusion process)\n","        noisy_actions = noise_scheduler.add_noise(\n","            naction, noise, timesteps)\n","        noisy_actions = noisy_actions.to(device, dtype=torch.float)\n","        obs_cond = obs_cond.to(device, dtype=torch.float)\n","        # predict the noise residual\n","        noise_pred = noise_pred_net(\n","            noisy_actions, timesteps, global_cond=obs_cond)\n","\n","        # L2 loss\n","        loss = nn.functional.mse_loss(noise_pred, noise)\n","\n","        # optimize\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        # step lr scheduler every batch\n","        # this is different from standard pytorch behavior\n","        lr_scheduler.step()\n","\n","        # update Exponential Moving Average of the model weights\n","        ema.step(nets)\n","\n","        # logging\n","        loss_cpu = loss.item()\n","        epoch_loss.append(loss_cpu)\n","    cur_loss = np.mean(epoch_loss)\n","    print(\"# epoch, loss: \", epoch_idx, cur_loss)\n","    if cur_loss < min_loss:\n","        torch.save({\n","            'epoch': epoch_idx,\n","            'model_state_dict': nets.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': cur_loss,\n","            }, save_path)\n","        min_loss = cur_loss\n","        print(\"A checkpoint is saved at # epoch! \", epoch_idx)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.012738244952978912\n"]}],"source":["import torch\n","checkpoint = torch.load('/home/foamlab/nw/save/tennis_sphere.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg","timestamp":1698254384773}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
