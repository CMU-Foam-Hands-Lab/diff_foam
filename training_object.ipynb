{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QwO2gAgiJS2","outputId":"eb446946-0d27-4753-b593-258fbae48d7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.10.12\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch==1.13.1 in ./.venv/lib/python3.10/site-packages (1.13.1)\n","Requirement already satisfied: torchvision==0.14.1 in ./.venv/lib/python3.10/site-packages (0.14.1)\n","Collecting diffusers==0.11.1\n","  Using cached diffusers-0.11.1-py3-none-any.whl (524 kB)\n","Requirement already satisfied: scikit-image==0.19.3 in ./.venv/lib/python3.10/site-packages (0.19.3)\n","Requirement already satisfied: scikit-video==1.1.11 in ./.venv/lib/python3.10/site-packages (1.1.11)\n","Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (4.12.2)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (8.5.0.96)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.10/site-packages (from torch==1.13.1) (11.10.3.66)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision==0.14.1) (10.4.0)\n","Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from torchvision==0.14.1) (2.32.3)\n","Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision==0.14.1) (1.26.4)\n","Requirement already satisfied: huggingface-hub>=0.10.0 in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (0.24.5)\n","Requirement already satisfied: importlib-metadata in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (8.2.0)\n","Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (2024.7.24)\n","Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from diffusers==0.11.1) (3.15.4)\n","Requirement already satisfied: imageio>=2.4.1 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (2.34.2)\n","Requirement already satisfied: networkx>=2.2 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (3.3)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (24.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.6.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (2024.7.24)\n","Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.10/site-packages (from scikit-image==0.19.3) (1.14.0)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (65.5.0)\n","Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.44.0)\n","Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (6.0.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (2024.6.1)\n","Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.66.5)\n","Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.11.1) (3.19.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2.2.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (3.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->torchvision==0.14.1) (2024.7.4)\n","Installing collected packages: diffusers\n","  Attempting uninstall: diffusers\n","    Found existing installation: diffusers 0.30.0\n","    Uninstalling diffusers-0.30.0:\n","      Successfully uninstalled diffusers-0.30.0\n","Successfully installed diffusers-0.11.1\n","Requirement already satisfied: diffusers[torch] in ./.venv/lib/python3.10/site-packages (0.11.1)\n","Collecting diffusers[torch]\n","  Using cached diffusers-0.30.0-py3-none-any.whl (2.6 MB)\n","Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (10.4.0)\n","Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (1.26.4)\n","Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (0.4.4)\n","Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (2024.7.24)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (0.24.5)\n","Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (2.32.3)\n","Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (3.15.4)\n","Requirement already satisfied: importlib-metadata in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (8.2.0)\n","Requirement already satisfied: accelerate>=0.31.0 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (0.33.0)\n","Requirement already satisfied: torch>=1.4 in ./.venv/lib/python3.10/site-packages (from diffusers[torch]) (1.13.1)\n","Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate>=0.31.0->diffusers[torch]) (6.0.0)\n","Requirement already satisfied: pyyaml in ./.venv/lib/python3.10/site-packages (from accelerate>=0.31.0->diffusers[torch]) (6.0.1)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from accelerate>=0.31.0->diffusers[torch]) (24.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (2024.6.1)\n","Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers[torch]) (4.12.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.10/site-packages (from torch>=1.4->diffusers[torch]) (11.10.3.66)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->diffusers[torch]) (65.5.0)\n","Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->diffusers[torch]) (0.44.0)\n","Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata->diffusers[torch]) (3.19.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (3.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (3.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (2024.7.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->diffusers[torch]) (2.2.2)\n","Installing collected packages: diffusers\n","  Attempting uninstall: diffusers\n","    Found existing installation: diffusers 0.11.1\n","    Uninstalling diffusers-0.11.1:\n","      Successfully uninstalled diffusers-0.11.1\n","Successfully installed diffusers-0.30.0\n","Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.19.3 in ./.venv/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n"]}],"source":["!python --version\n","!pip3 install setuptools==65.5.0 pip==22 > /dev/null\n","!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.11.1 scikit-image==0.19.3 scikit-video==1.1.11\n","!pip3 install --upgrade diffusers[torch]\n","!pip3 install opencv-python"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VrX4VTl5pYNq"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available: True\n","Number of GPUs: 1\n","CUDA version: 11.7\n","8500\n"]}],"source":["#### **Imports**\n","from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n","import numpy as np\n","import math\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import collections\n","from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n","from diffusers.training_utils import EMAModel\n","from diffusers.optimization import get_scheduler\n","from tqdm.auto import tqdm\n","import torchvision.transforms as transforms\n","import pickle\n","import cv2\n","import os\n","\n","print(f\"CUDA is available: {torch.cuda.is_available()}\")\n","print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n","print(f\"CUDA version: {torch.version.cuda}\")\n","print(torch.backends.cudnn.version())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NK9ywrFUZedr"},"outputs":[],"source":["### data loading & saving\n","# tasks = [\"data-folder-name\"]\n","# task_name = \"task-name\"\n","tasks = [\"pkls\"]\n","task_name = \"dice\"\n","data_path = \"mydata/dice/\"\n","# save_path = \"save/\" + task_name + \"_correct.pt\"\n","dev = 'cuda'"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vHepJOFBucwg"},"outputs":[],"source":["### **Dataset**\n","# Defines `FoamDataset` and helper functions\n","# The dataset class\n","# Load data ((image, agent_pos), action) from pkl file\n","# Normalizes each dimension of agent_pos and action to [-1,1]\n","# Returns\n","# All possible segments with length `pred_horizon`\n","# Pads the beginning and the end of each episode with repetition\n","# key `image`: shape (obs_hoirzon, 3, 480, 640)\n","# key `agent_pos`: shape (obs_hoirzon, 23) # joint position for hand and arm\n","# key `action`: shape (pred_horizon, 23) # joint position for hand and arm\n","\n","def create_sample_indices(\n","        episode_ends:np.ndarray, sequence_length:int,\n","        pad_before: int=0, pad_after: int=0):\n","    indices = list()\n","    for i in range(len(episode_ends)):\n","        start_idx = 0\n","        if i > 0:\n","            start_idx = episode_ends[i-1]\n","        end_idx = episode_ends[i]\n","        episode_length = end_idx - start_idx\n","\n","        min_start = -pad_before\n","        max_start = episode_length - sequence_length + pad_after\n","\n","        # range stops one idx before end\n","        for idx in range(min_start, max_start+1):\n","            buffer_start_idx = max(idx, 0) + start_idx\n","            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n","            start_offset = buffer_start_idx - (idx+start_idx)\n","            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n","            sample_start_idx = 0 + start_offset\n","            sample_end_idx = sequence_length - end_offset\n","            indices.append([\n","                buffer_start_idx, buffer_end_idx,\n","                sample_start_idx, sample_end_idx])\n","    indices = np.array(indices)\n","    return indices\n","\n","\n","def sample_sequence(train_data, sequence_length,\n","                    buffer_start_idx, buffer_end_idx,\n","                    sample_start_idx, sample_end_idx):\n","    result = dict()\n","    for key, input_arr in train_data.items():\n","        sample = input_arr[buffer_start_idx:buffer_end_idx]\n","        data = sample\n","        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n","            data = np.zeros(\n","                shape=(sequence_length,) + input_arr.shape[1:],\n","                dtype=input_arr.dtype)\n","            if sample_start_idx > 0:\n","                data[:sample_start_idx] = sample[0]\n","            if sample_end_idx < sequence_length:\n","                data[sample_end_idx:] = sample[-1]\n","            data[sample_start_idx:sample_end_idx] = sample\n","        result[key] = data\n","    return result\n","\n","\n","def combine_data(cmd_hand, cmd_arm):\n","    assert cmd_hand.shape[0] == cmd_arm.shape[0], \"inconsistent number of samples\"\n","    combined_data = np.concatenate((cmd_hand, cmd_arm), axis=1)\n","    return combined_data\n","\n","def get_data_stats():\n","    min_values = np.array([0.000] * 23)  \n","    max_values = np.array([1.000] * 23) \n","    min_values[3] = -1.000  \n","    max_values[3] = 1.000   \n","    min_values[11] = -1.000  \n","    max_values[11] = 1.000   \n","    min_values[16:] = np.array([-0.28, -0.78, -1.19, 0.13, -0.15, 0.14, -2.79])\n","    max_values[16:] = np.array([0.66,  0.20,  0.17,  1.67, 1.06,  1.68, -0.71])\n","\n","    stats = {\n","        'min': min_values,\n","        'max': max_values\n","    }\n","    return stats\n","\n","def normalize_data(data, stats):\n","    ndata = np.zeros_like(data)\n","    \n","    # Normalize first 16 dimensions\n","    ndata[:, [3, 11]] = data[:, [3, 11]]\n","    \n","    # Normalize the other 14 dimensions to [-1, 1]\n","    for i in range(16):\n","        if i not in [3, 11]:  # dimensions 4 and 12\n","            ndata[:, i] = (data[:, i] * 2) - 1  # Mapping from [0, 1] to [-1, 1]\n","    \n","    # Normalize the last 7 dimensions\n","    last_7_start = 16\n","    last_7_end = 23\n","    \n","    for i in range(last_7_start, last_7_end):\n","        range_ = stats['max'][i] - stats['min'][i]\n","        ndata[:, i] = 2 * (data[:, i] - stats['min'][i]) / range_ - 1  # Mapping from [min, max] to [-1, 1]\n","    \n","    return ndata\n","\n","def unnormalize_data(ndata, stats):\n","    data = np.zeros_like(ndata)\n","    \n","    # Unnormalize first 16 dimensions\n","    data[:, [3, 11]] = ndata[:, [3, 11]]\n","    \n","    # Unnormalize the other 14 dimensions from [-1, 1] to [0, 1]\n","    for i in range(16):\n","        if i not in [3, 11]:  # dimensions 4 and 12\n","            data[:, i] = (ndata[:, i] + 1) / 2  # Mapping from [-1, 1] to [0, 1]\n","    \n","    # Unnormalize the last 7 dimensions\n","    last_7_start = 16\n","    last_7_end = 23\n","    \n","    for i in range(last_7_start, last_7_end):\n","        range_ = stats['max'][i] - stats['min'][i]\n","        data[:, i] = (ndata[:, i] + 1) / 2 * range_ + stats['min'][i]  # Restore to original range\n","    \n","    return data\n","\n","\n","def normalize_images(images):\n","    # resize image to (120, 160)\n","    # nomalize to [0,1]\n","    nimages = images / 255.0\n","    return nimages\n","\n","def add_noise(inputs):\n","     noise = torch.randn_like(inputs) * 0.2 - 0.1 #[-0.1, 0.1]\n","     return torch.clamp(inputs + noise, min = -1.0, max = 1.0)\n","\n","transforms_noise = transforms.Compose([\n","    # transforms.RandomRotation(30),\n","    # transforms.RandomCrop(size=(216, 288)),\n","    transforms.ColorJitter(brightness=0.5, contrast=1, saturation=0.1, hue=0.5),\n","])\n","\n","# dataset\n","class FoamDataset(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 data_folder: list,\n","                 pred_horizon: int,\n","                 obs_horizon: int,\n","                 action_horizon: int):\n","\n","        # load all traj's data\n","        image_data = []\n","        actions = []\n","        states = []\n","        episode_ends = [] # the end idx of each traj\n","        cur_cnt = 0\n","        for folder in data_folder:\n","          data_list = sorted([data for data in os.listdir(folder) if data.endswith(\".pkl\")])\n","          for i in range(len(data_list)):\n","              with open(folder + data_list[i], 'rb') as f:\n","                data = pickle.load(f)\n","                image_data.append(np.array(data['image'])) # (240, 320, 3) # 0 - 255\n","                # print(\"image_data shape:\", image_data[0].shape)  \n","                cmd_hand = np.array(data['cmd_hand'])  # shape (N, 16)\n","                cmd_arm = np.array(data['cmd_xarm'])   # shape (N, 7)\n","                # print(cmd_hand.shape, cmd_arm.shape)\n","                combined_data = np.concatenate((cmd_hand, cmd_arm), axis=1)\n","                # print(combined_data.shape)\n","                cur_state = np.array(combined_data)\n","\n","                next_state = np.zeros_like(cur_state)\n","                next_state[:-1,:] = cur_state[1:,:]\n","                # 将cur_state数组中从第二行（索引为1）到最后一行的所有行的值，赋值给next_state数组中从第一行（索引为0）到倒数第二行的所有行。\n","                # 通俗来说，就是将cur_state中往后移一行的数据复制给next_state，从而实现next_state的平移。\n","                next_state[-1,:] = next_state[-2,:]\n","                # 将next_state数组中倒数第二行的值复制给最后一行。也就是说，next_state的最后一行将与倒数第二行的值相同。\n","                action = next_state # predict next timestamp's pos as the action\n","\n","                states.append(cur_state)\n","                actions.append(action)\n","                cur_cnt += len(cur_state)\n","                episode_ends.append(cur_cnt)\n","\n","        print(\"Concatenating images...\")\n","        image_data = np.concatenate(image_data)\n","        episode_ends = np.array(episode_ends)\n","        print(\"Concatenating actions...\")\n","        actions = np.concatenate(actions)\n","        print(\"Concatenating states...\")\n","        states = np.concatenate(states)\n","\n","        # float32, [0,1], (N, 480, 640, 3) (N, 480, 640, 3)\n","        # print(\"Normalizing images...\")\n","        train_image_data = image_data\n","        print(\"Train image size: \", train_image_data.shape)  # Train image size:  (4279, 240, 320, 3)\n","        print(\"Swaping image idex...\")\n","        train_image_data = np.moveaxis(train_image_data, -1,1) # (N, 3, 240, 320)\n","\n","        # (N, D)\n","        train_data = {\n","            'agent_pos': states,\n","            'action': actions\n","        }\n","\n","        # compute start and end of each state-action sequence\n","        # also handles padding\n","        print(\"Creating sample indices...\")\n","        indices = create_sample_indices(\n","            episode_ends=episode_ends,\n","            sequence_length=pred_horizon,\n","            pad_before=obs_horizon-1,\n","            pad_after=action_horizon-1)\n","\n","        # compute statistics and normalized data to [-1,1]\n","        stats = dict()\n","        normalized_train_data = dict()\n","        for key, data in train_data.items():\n","            stats[key] = get_data_stats()\n","            normalized_train_data[key] = normalize_data(data, stats[key])\n","\n","        # images are already normalized\n","        normalized_train_data['image'] = train_image_data\n","\n","        self.indices = indices\n","        self.stats = stats\n","        self.normalized_train_data = normalized_train_data\n","        self.pred_horizon = pred_horizon\n","        self.action_horizon = action_horizon\n","        self.obs_horizon = obs_horizon\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        # get the start/end indices for this datapoint\n","        buffer_start_idx, buffer_end_idx, \\\n","            sample_start_idx, sample_end_idx = self.indices[idx]\n","\n","        # get nomralized data using these indices\n","        nsample = sample_sequence(\n","            train_data=self.normalized_train_data,\n","            sequence_length=self.pred_horizon,\n","            buffer_start_idx=buffer_start_idx,\n","            buffer_end_idx=buffer_end_idx,\n","            sample_start_idx=sample_start_idx,\n","            sample_end_idx=sample_end_idx\n","        )\n","\n","        # discard unused observations\n","        # we normalize images here!\n","        nsample['image'] = transforms_noise(torch.from_numpy(nsample['image'][:self.obs_horizon,:] / 255.0))\n","        nsample['agent_pos'] = add_noise(torch.from_numpy(nsample['agent_pos'][:self.obs_horizon,:]))\n","        # 从 nsample['agent_pos'] 中提取前 self.obs_horizon 行的数据。\n","        nsample['action'] = torch.from_numpy(nsample['action'][:self.pred_horizon,:])\n","        # 从 nsample['action'] 中提取前 self.pred_horizon 行的数据。\n","        return nsample\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9ZiHF3lzvB6k"},"outputs":[{"name":"stdout","output_type":"stream","text":["Concatenating images...\n","Concatenating actions...\n","Concatenating states...\n","Train image size:  (4279, 240, 320, 3)\n","Swaping image idex...\n","Creating sample indices...\n","stats: {'agent_pos': {'min': array([ 0.  ,  0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n","        0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.28, -0.78,\n","       -1.19,  0.13, -0.15,  0.14, -2.79]), 'max': array([ 1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,\n","        1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  0.66,  0.2 ,\n","        0.17,  1.67,  1.06,  1.68, -0.71])}, 'action': {'min': array([ 0.  ,  0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n","        0.  ,  0.  , -1.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.28, -0.78,\n","       -1.19,  0.13, -0.15,  0.14, -2.79]), 'max': array([ 1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,\n","        1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  1.  ,  0.66,  0.2 ,\n","        0.17,  1.67,  1.06,  1.68, -0.71])}}\n","Creating dataloader...\n"]}],"source":["### load data\n","import os\n","import pickle\n","\n","data_folder = []\n","for task in tasks:\n","    data_folder.append(data_path + task + '/')\n","\n","# parameters\n","pred_horizon = 16\n","obs_horizon = 2\n","action_horizon = 8\n","#|o|o|                             observations: 2\n","#| |a|a|a|a|a|a|a|a|               actions executed: 8\n","#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n","\n","# create dataset from file\n","dataset = FoamDataset(\n","    data_folder=data_folder,\n","    pred_horizon=pred_horizon,\n","    obs_horizon=obs_horizon,\n","    action_horizon=action_horizon\n",")\n","# save training data statistics (min, max) for each dim\n","stats = dataset.stats\n","print(\"stats:\", stats)\n","\n","# create dataloader\n","print(\"Creating dataloader...\")\n","dataloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=64,\n","    num_workers=2,\n","    shuffle=True,\n","    # accelerate cpu-gpu transfer\n","    pin_memory=False,\n","    # don't kill worker process afte each epoch\n","    persistent_workers=False\n",")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"X-XRB_g3vsgf"},"outputs":[],"source":["#@markdown ### **Network**\n","#@markdown\n","#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n","#@markdown as the noies prediction network\n","#@markdown\n","#@markdown Components\n","#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n","#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n","#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n","#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n","#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n","#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n","#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n","\n","class SinusoidalPosEmb(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        device = x.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = x[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","        return emb\n","\n","\n","class Downsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Upsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Conv1dBlock(nn.Module):\n","    '''\n","        Conv1d --> GroupNorm --> Mish\n","    '''\n","\n","    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n","        super().__init__()\n","\n","        self.block = nn.Sequential(\n","            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n","            nn.GroupNorm(n_groups, out_channels),\n","            nn.Mish(),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class ConditionalResidualBlock1D(nn.Module):\n","    def __init__(self,\n","            in_channels,\n","            out_channels,\n","            cond_dim,\n","            kernel_size=3,\n","            n_groups=8):\n","        super().__init__()\n","\n","        self.blocks = nn.ModuleList([\n","            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n","            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n","        ])\n","\n","        # FiLM modulation https://arxiv.org/abs/1709.07871\n","        # predicts per-channel scale and bias\n","        cond_channels = out_channels * 2\n","        self.out_channels = out_channels\n","        self.cond_encoder = nn.Sequential(\n","            nn.Mish(),\n","            nn.Linear(cond_dim, cond_channels),\n","            nn.Unflatten(-1, (-1, 1))\n","        )\n","\n","        # make sure dimensions compatible\n","        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n","            if in_channels != out_channels else nn.Identity()\n","\n","    def forward(self, x, cond):\n","        '''\n","            x : [ batch_size x in_channels x horizon ]\n","            cond : [ batch_size x cond_dim]\n","\n","            returns:\n","            out : [ batch_size x out_channels x horizon ]\n","        '''\n","        out = self.blocks[0](x)\n","        embed = self.cond_encoder(cond)\n","\n","        embed = embed.reshape(\n","            embed.shape[0], 2, self.out_channels, 1)\n","        scale = embed[:,0,...]\n","        bias = embed[:,1,...]\n","        out = scale * out + bias\n","\n","        out = self.blocks[1](out)\n","        out = out + self.residual_conv(x)\n","        return out\n","\n","\n","class ConditionalUnet1D(nn.Module):\n","    def __init__(self,\n","        input_dim,\n","        global_cond_dim,\n","        diffusion_step_embed_dim=256,\n","        down_dims=[256,512,1024],\n","        kernel_size=5,\n","        n_groups=8\n","        ):\n","        \"\"\"\n","        input_dim: Dim of actions.\n","        global_cond_dim: Dim of global conditioning applied with FiLM\n","          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n","        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n","        down_dims: Channel size for each UNet level.\n","          The length of this array determines numebr of levels.\n","        kernel_size: Conv kernel size\n","        n_groups: Number of groups for GroupNorm\n","        \"\"\"\n","\n","        super().__init__()\n","        all_dims = [input_dim] + list(down_dims)\n","        start_dim = down_dims[0]\n","\n","        dsed = diffusion_step_embed_dim\n","        diffusion_step_encoder = nn.Sequential(\n","            SinusoidalPosEmb(dsed),\n","            nn.Linear(dsed, dsed * 4),\n","            nn.Mish(),\n","            nn.Linear(dsed * 4, dsed),\n","        )\n","        cond_dim = dsed + global_cond_dim\n","\n","        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n","        mid_dim = all_dims[-1]\n","        self.mid_modules = nn.ModuleList([\n","            ConditionalResidualBlock1D(\n","                mid_dim, mid_dim, cond_dim=cond_dim,\n","                kernel_size=kernel_size, n_groups=n_groups\n","            ),\n","            ConditionalResidualBlock1D(\n","                mid_dim, mid_dim, cond_dim=cond_dim,\n","                kernel_size=kernel_size, n_groups=n_groups\n","            ),\n","        ])\n","\n","        down_modules = nn.ModuleList([])\n","        for ind, (dim_in, dim_out) in enumerate(in_out):\n","            is_last = ind >= (len(in_out) - 1)\n","            down_modules.append(nn.ModuleList([\n","                ConditionalResidualBlock1D(\n","                    dim_in, dim_out, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                ConditionalResidualBlock1D(\n","                    dim_out, dim_out, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                Downsample1d(dim_out) if not is_last else nn.Identity()\n","            ]))\n","\n","        up_modules = nn.ModuleList([])\n","        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n","            is_last = ind >= (len(in_out) - 1)\n","            up_modules.append(nn.ModuleList([\n","                ConditionalResidualBlock1D(\n","                    dim_out*2, dim_in, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                ConditionalResidualBlock1D(\n","                    dim_in, dim_in, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                Upsample1d(dim_in) if not is_last else nn.Identity()\n","            ]))\n","\n","        final_conv = nn.Sequential(\n","            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n","            nn.Conv1d(start_dim, input_dim, 1),\n","        )\n","\n","        self.diffusion_step_encoder = diffusion_step_encoder\n","        self.up_modules = up_modules\n","        self.down_modules = down_modules\n","        self.final_conv = final_conv\n","\n","        print(\"number of parameters: {:e}\".format(\n","            sum(p.numel() for p in self.parameters()))\n","        )\n","\n","    def forward(self,\n","            sample: torch.Tensor,\n","            timestep: Union[torch.Tensor, float, int],\n","            global_cond=None):\n","        \"\"\"\n","        x: (B,T,input_dim)\n","        timestep: (B,) or int, diffusion step\n","        global_cond: (B,global_cond_dim)\n","        output: (B,T,input_dim)\n","        \"\"\"\n","        # (B,T,C)\n","        sample = sample.moveaxis(-1,-2)\n","        # (B,C,T)\n","\n","        # 1. time\n","        timesteps = timestep\n","        if not torch.is_tensor(timesteps):\n","            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n","            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n","        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n","            timesteps = timesteps[None].to(sample.device)\n","        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n","        timesteps = timesteps.expand(sample.shape[0])\n","\n","        global_feature = self.diffusion_step_encoder(timesteps)\n","\n","        if global_cond is not None:\n","            global_feature = torch.cat([\n","                global_feature, global_cond\n","            ], axis=-1)\n","\n","        x = sample\n","        h = []\n","        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n","            x = resnet(x, global_feature)\n","            x = resnet2(x, global_feature)\n","            h.append(x)\n","            x = downsample(x)\n","\n","        for mid_module in self.mid_modules:\n","            x = mid_module(x, global_feature)\n","\n","        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n","            x = torch.cat((x, h.pop()), dim=1)\n","            x = resnet(x, global_feature)\n","            x = resnet2(x, global_feature)\n","            x = upsample(x)\n","\n","        x = self.final_conv(x)\n","\n","        # (B,C,T)\n","        x = x.moveaxis(-1,-2)\n","        # (B,T,C)\n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yXq4r744aMh1"},"outputs":[],"source":["#### **Vision Encoder**\n","# Defines helper functions:\n","# `get_resnet` to initialize standard ResNet vision encoder\n","# `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n","\n","def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n","    \"\"\"\n","    name: resnet18, resnet34, resnet50\n","    weights: \"IMAGENET1K_V1\", None\n","    \"\"\"\n","    # Use standard ResNet implementation from torchvision\n","    func = getattr(torchvision.models, name)\n","    resnet = func(weights=weights, **kwargs)\n","\n","    # remove the final fully connected layer\n","    # for resnet18, the output dim should be 512\n","    resnet.fc = torch.nn.Identity()\n","    return resnet\n","\n","\n","def replace_submodules(\n","        root_module: nn.Module,\n","        predicate: Callable[[nn.Module], bool],\n","        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n","    \"\"\"\n","    Replace all submodules selected by the predicate with\n","    the output of func.\n","\n","    predicate: Return true if the module is to be replaced.\n","    func: Return new module to use.\n","    \"\"\"\n","    if predicate(root_module):\n","        return func(root_module)\n","\n","    bn_list = [k.split('.') for k, m\n","        in root_module.named_modules(remove_duplicate=True)\n","        if predicate(m)]\n","    for *parent, k in bn_list:\n","        parent_module = root_module\n","        if len(parent) > 0:\n","            parent_module = root_module.get_submodule('.'.join(parent))\n","        if isinstance(parent_module, nn.Sequential):\n","            src_module = parent_module[int(k)]\n","        else:\n","            src_module = getattr(parent_module, k)\n","        tgt_module = func(src_module)\n","        if isinstance(parent_module, nn.Sequential):\n","            parent_module[int(k)] = tgt_module\n","        else:\n","            setattr(parent_module, k, tgt_module)\n","    # verify that all modules are replaced\n","    bn_list = [k.split('.') for k, m\n","        in root_module.named_modules(remove_duplicate=True)\n","        if predicate(m)]\n","    assert len(bn_list) == 0\n","    return root_module\n","\n","def replace_bn_with_gn(\n","    root_module: nn.Module,\n","    features_per_group: int=16) -> nn.Module:\n","    \"\"\"\n","    Relace all BatchNorm layers with GroupNorm.\n","    \"\"\"\n","    replace_submodules(\n","        root_module=root_module,\n","        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n","        func=lambda x: nn.GroupNorm(\n","            num_groups=x.num_features//features_per_group,\n","            num_channels=x.num_features)\n","    )\n","    return root_module\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4APZkqh336-M","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["number of parameters: 8.058703e+07\n","torch.Size([1, 2, 535])\n"]}],"source":["#### **Network Demo**\n","\n","# construct ResNet18 encoder\n","# if you have multiple camera views, use seperate encoder weights for each view.\n","vision_encoder = get_resnet('resnet18')\n","\n","# IMPORTANT!\n","# replace all BatchNorm with GroupNorm to work with EMA\n","# performance will tank if you forget to do this!\n","vision_encoder = replace_bn_with_gn(vision_encoder)\n","\n","# ResNet18 has output dim of 512\n","vision_feature_dim = 512\n","# agent_pos is 32 dimensional\n","lowdim_obs_dim = 23\n","# observation feature has 512+32 dims in total per step\n","obs_dim = vision_feature_dim + lowdim_obs_dim\n","action_dim = 23\n","\n","# create network object\n","noise_pred_net = ConditionalUnet1D(\n","    input_dim=action_dim,\n","    global_cond_dim=obs_dim*obs_horizon\n",")\n","\n","# the final arch has 2 parts\n","nets = nn.ModuleDict({\n","    'vision_encoder': vision_encoder,\n","    'noise_pred_net': noise_pred_net\n","})\n","\n","# demo\n","with torch.no_grad():\n","    # example inputs\n","    image = torch.zeros((1, obs_horizon,3,480,640))\n","    agent_pos = torch.zeros((1, obs_horizon, lowdim_obs_dim))\n","    # vision encoder\n","    image_features = nets['vision_encoder'](\n","        image.flatten(end_dim=1))\n","    # (2,512)\n","    image_features = image_features.reshape(*image.shape[:2],-1)\n","    # (1,2,512)\n","    obs = torch.cat([image_features, agent_pos],dim=-1)\n","    # (1,2,512+23)\n","    print(obs.shape)\n","\n","    noised_action = torch.randn((1, pred_horizon, action_dim))\n","    diffusion_iter = torch.zeros((1,))\n","\n","    # the noise prediction network\n","    # takes noisy action, diffusion iteration and observation as input\n","    # predicts the noise added to action\n","    noise = nets['noise_pred_net'](\n","        sample=noised_action,\n","        timestep=diffusion_iter,\n","        global_cond=obs.flatten(start_dim=1))\n","\n","    # illustration of removing noise\n","    # the actual noise removal is performed by NoiseScheduler\n","    # and is dependent on the diffusion noise schedule\n","    denoised_action = noised_action - noise\n","\n","# for this demo, we use DDPMScheduler with 100 diffusion iterations\n","num_diffusion_iters = 100\n","noise_scheduler = DDPMScheduler(\n","    num_train_timesteps=num_diffusion_iters,\n","    # the choise of beta schedule has big impact on performance\n","    # we found squared cosine works the best\n","    beta_schedule='squaredcos_cap_v2',\n","    # clip output to [-1,1] to improve stability\n","    clip_sample=True,\n","    # our network predicts noise (instead of denoised action)\n","    prediction_type='epsilon'\n",")\n","\n","# device transfer\n","device = torch.device(dev) #'cuda' # error on nvidia version\n","_ = nets.to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No checkpoint found, starting training from scratch.\n"]},{"name":"stderr","output_type":"stream","text":["/home/foamlab/nw/.venv/lib/python3.10/site-packages/diffusers/training_utils.py:403: FutureWarning: Passing a `torch.nn.Module` to `ExponentialMovingAverage.step` is deprecated. Please pass the parameters of the module instead.\n","  deprecate(\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 0, loss: 1.0248\n","A checkpoint is saved at epoch 0!\n","# epoch 1, loss: 0.9101\n","A checkpoint is saved at epoch 1!\n","# epoch 2, loss: 0.6766\n","A checkpoint is saved at epoch 2!\n","# epoch 3, loss: 0.4976\n","A checkpoint is saved at epoch 3!\n","# epoch 4, loss: 0.3716\n","A checkpoint is saved at epoch 4!\n","# epoch 5, loss: 0.2791\n","A checkpoint is saved at epoch 5!\n","# epoch 6, loss: 0.2141\n","A checkpoint is saved at epoch 6!\n","# epoch 7, loss: 0.1834\n","A checkpoint is saved at epoch 7!\n","# epoch 8, loss: 0.1512\n","A checkpoint is saved at epoch 8!\n","# epoch 9, loss: 0.1276\n","A checkpoint is saved at epoch 9!\n","# epoch 10, loss: 0.1177\n","A checkpoint is saved at epoch 10!\n","# epoch 11, loss: 0.1106\n","A checkpoint is saved at epoch 11!\n","# epoch 12, loss: 0.1028\n","A checkpoint is saved at epoch 12!\n","# epoch 13, loss: 0.0959\n","A checkpoint is saved at epoch 13!\n","# epoch 14, loss: 0.0927\n","A checkpoint is saved at epoch 14!\n","# epoch 15, loss: 0.0886\n","A checkpoint is saved at epoch 15!\n","# epoch 16, loss: 0.0852\n","A checkpoint is saved at epoch 16!\n","# epoch 17, loss: 0.0794\n","A checkpoint is saved at epoch 17!\n","# epoch 18, loss: 0.0778\n","A checkpoint is saved at epoch 18!\n","# epoch 19, loss: 0.0794\n","# epoch 20, loss: 0.0736\n","A checkpoint is saved at epoch 20!\n","# epoch 21, loss: 0.0711\n","A checkpoint is saved at epoch 21!\n","# epoch 22, loss: 0.0703\n","A checkpoint is saved at epoch 22!\n","# epoch 23, loss: 0.0687\n","A checkpoint is saved at epoch 23!\n","# epoch 24, loss: 0.0667\n","A checkpoint is saved at epoch 24!\n","# epoch 25, loss: 0.0643\n","A checkpoint is saved at epoch 25!\n","# epoch 26, loss: 0.0622\n","A checkpoint is saved at epoch 26!\n","# epoch 27, loss: 0.0626\n","# epoch 28, loss: 0.0609\n","A checkpoint is saved at epoch 28!\n","# epoch 29, loss: 0.0576\n","A checkpoint is saved at epoch 29!\n","# epoch 30, loss: 0.0561\n","A checkpoint is saved at epoch 30!\n","# epoch 31, loss: 0.0582\n","# epoch 32, loss: 0.0555\n","A checkpoint is saved at epoch 32!\n","# epoch 33, loss: 0.0545\n","A checkpoint is saved at epoch 33!\n","# epoch 34, loss: 0.0538\n","A checkpoint is saved at epoch 34!\n","# epoch 35, loss: 0.0500\n","A checkpoint is saved at epoch 35!\n","# epoch 36, loss: 0.0516\n","# epoch 37, loss: 0.0509\n","# epoch 38, loss: 0.0505\n","# epoch 39, loss: 0.0483\n","A checkpoint is saved at epoch 39!\n","# epoch 40, loss: 0.0484\n","# epoch 41, loss: 0.0446\n","A checkpoint is saved at epoch 41!\n","# epoch 42, loss: 0.0490\n","# epoch 43, loss: 0.0496\n","# epoch 44, loss: 0.0474\n","# epoch 45, loss: 0.0464\n","# epoch 46, loss: 0.0424\n","A checkpoint is saved at epoch 46!\n","# epoch 47, loss: 0.0445\n","# epoch 48, loss: 0.0427\n","# epoch 49, loss: 0.0421\n","A checkpoint is saved at epoch 49!\n","# epoch 50, loss: 0.0429\n","# epoch 51, loss: 0.0441\n","# epoch 52, loss: 0.0422\n","# epoch 53, loss: 0.0410\n","A checkpoint is saved at epoch 53!\n","# epoch 54, loss: 0.0424\n","# epoch 55, loss: 0.0429\n","# epoch 56, loss: 0.0408\n","A checkpoint is saved at epoch 56!\n","# epoch 57, loss: 0.0400\n","A checkpoint is saved at epoch 57!\n","# epoch 58, loss: 0.0387\n","A checkpoint is saved at epoch 58!\n","# epoch 59, loss: 0.0397\n","# epoch 60, loss: 0.0408\n","# epoch 61, loss: 0.0380\n","A checkpoint is saved at epoch 61!\n","# epoch 62, loss: 0.0388\n","# epoch 63, loss: 0.0379\n","A checkpoint is saved at epoch 63!\n","# epoch 64, loss: 0.0356\n","A checkpoint is saved at epoch 64!\n","# epoch 65, loss: 0.0376\n","# epoch 66, loss: 0.0381\n","# epoch 67, loss: 0.0368\n","# epoch 68, loss: 0.0351\n","A checkpoint is saved at epoch 68!\n","# epoch 69, loss: 0.0343\n","A checkpoint is saved at epoch 69!\n","# epoch 70, loss: 0.0352\n","# epoch 71, loss: 0.0356\n","# epoch 72, loss: 0.0359\n","# epoch 73, loss: 0.0331\n","A checkpoint is saved at epoch 73!\n","# epoch 74, loss: 0.0338\n","# epoch 75, loss: 0.0331\n","# epoch 76, loss: 0.0325\n","A checkpoint is saved at epoch 76!\n","# epoch 77, loss: 0.0323\n","A checkpoint is saved at epoch 77!\n","# epoch 78, loss: 0.0325\n","# epoch 79, loss: 0.0317\n","A checkpoint is saved at epoch 79!\n","# epoch 80, loss: 0.0337\n","# epoch 81, loss: 0.0330\n","# epoch 82, loss: 0.0304\n","A checkpoint is saved at epoch 82!\n","# epoch 83, loss: 0.0309\n","# epoch 84, loss: 0.0313\n","# epoch 85, loss: 0.0295\n","A checkpoint is saved at epoch 85!\n","# epoch 86, loss: 0.0292\n","A checkpoint is saved at epoch 86!\n","# epoch 87, loss: 0.0305\n","# epoch 88, loss: 0.0308\n","# epoch 89, loss: 0.0320\n","# epoch 90, loss: 0.0281\n","A checkpoint is saved at epoch 90!\n","# epoch 91, loss: 0.0309\n","# epoch 92, loss: 0.0307\n","# epoch 93, loss: 0.0288\n","# epoch 94, loss: 0.0269\n","A checkpoint is saved at epoch 94!\n","# epoch 95, loss: 0.0283\n","# epoch 96, loss: 0.0289\n","# epoch 97, loss: 0.0285\n","# epoch 98, loss: 0.0264\n","A checkpoint is saved at epoch 98!\n","# epoch 99, loss: 0.0275\n","# epoch 100, loss: 0.0281\n","# epoch 101, loss: 0.0274\n","# epoch 102, loss: 0.0273\n","# epoch 103, loss: 0.0246\n","A checkpoint is saved at epoch 103!\n","# epoch 104, loss: 0.0253\n","# epoch 105, loss: 0.0263\n","# epoch 106, loss: 0.0278\n","# epoch 107, loss: 0.0270\n","# epoch 108, loss: 0.0249\n","# epoch 109, loss: 0.0239\n","A checkpoint is saved at epoch 109!\n","# epoch 110, loss: 0.0236\n","A checkpoint is saved at epoch 110!\n","# epoch 111, loss: 0.0242\n","# epoch 112, loss: 0.0242\n","# epoch 113, loss: 0.0234\n","A checkpoint is saved at epoch 113!\n","# epoch 114, loss: 0.0233\n","A checkpoint is saved at epoch 114!\n","# epoch 115, loss: 0.0235\n","# epoch 116, loss: 0.0240\n","# epoch 117, loss: 0.0243\n","# epoch 118, loss: 0.0222\n","A checkpoint is saved at epoch 118!\n","# epoch 119, loss: 0.0226\n","# epoch 120, loss: 0.0210\n","A checkpoint is saved at epoch 120!\n","# epoch 121, loss: 0.0223\n","# epoch 122, loss: 0.0207\n","A checkpoint is saved at epoch 122!\n","# epoch 123, loss: 0.0219\n","# epoch 124, loss: 0.0201\n","A checkpoint is saved at epoch 124!\n","# epoch 125, loss: 0.0220\n","# epoch 126, loss: 0.0202\n","# epoch 127, loss: 0.0207\n","# epoch 128, loss: 0.0207\n","# epoch 129, loss: 0.0203\n","# epoch 130, loss: 0.0212\n","# epoch 131, loss: 0.0210\n","# epoch 132, loss: 0.0195\n","A checkpoint is saved at epoch 132!\n","# epoch 133, loss: 0.0181\n","A checkpoint is saved at epoch 133!\n","# epoch 134, loss: 0.0193\n","# epoch 135, loss: 0.0196\n","# epoch 136, loss: 0.0186\n","# epoch 137, loss: 0.0182\n","# epoch 138, loss: 0.0201\n","# epoch 139, loss: 0.0179\n","A checkpoint is saved at epoch 139!\n","# epoch 140, loss: 0.0198\n","# epoch 141, loss: 0.0188\n","# epoch 142, loss: 0.0178\n","A checkpoint is saved at epoch 142!\n","# epoch 143, loss: 0.0172\n","A checkpoint is saved at epoch 143!\n","# epoch 144, loss: 0.0169\n","A checkpoint is saved at epoch 144!\n","# epoch 145, loss: 0.0161\n","A checkpoint is saved at epoch 145!\n","# epoch 146, loss: 0.0178\n","# epoch 147, loss: 0.0171\n","# epoch 148, loss: 0.0174\n","# epoch 149, loss: 0.0162\n","# epoch 150, loss: 0.0159\n","A checkpoint is saved at epoch 150!\n","# epoch 151, loss: 0.0166\n","# epoch 152, loss: 0.0151\n","A checkpoint is saved at epoch 152!\n","# epoch 153, loss: 0.0157\n","# epoch 154, loss: 0.0152\n","# epoch 155, loss: 0.0156\n","# epoch 156, loss: 0.0147\n","A checkpoint is saved at epoch 156!\n","# epoch 157, loss: 0.0155\n","# epoch 158, loss: 0.0143\n","A checkpoint is saved at epoch 158!\n","# epoch 159, loss: 0.0154\n","# epoch 160, loss: 0.0146\n","# epoch 161, loss: 0.0144\n","# epoch 162, loss: 0.0142\n","A checkpoint is saved at epoch 162!\n","# epoch 163, loss: 0.0144\n","# epoch 164, loss: 0.0148\n","# epoch 165, loss: 0.0145\n","# epoch 166, loss: 0.0139\n","A checkpoint is saved at epoch 166!\n","# epoch 167, loss: 0.0135\n","A checkpoint is saved at epoch 167!\n","# epoch 168, loss: 0.0157\n","# epoch 169, loss: 0.0142\n","# epoch 170, loss: 0.0135\n","# epoch 171, loss: 0.0135\n","# epoch 172, loss: 0.0126\n","A checkpoint is saved at epoch 172!\n","# epoch 173, loss: 0.0134\n","# epoch 174, loss: 0.0137\n","# epoch 175, loss: 0.0138\n","# epoch 176, loss: 0.0131\n","# epoch 177, loss: 0.0129\n","# epoch 178, loss: 0.0137\n","# epoch 179, loss: 0.0130\n","# epoch 180, loss: 0.0134\n","# epoch 181, loss: 0.0126\n","# epoch 182, loss: 0.0129\n","# epoch 183, loss: 0.0129\n","# epoch 184, loss: 0.0122\n","A checkpoint is saved at epoch 184!\n","# epoch 185, loss: 0.0126\n","# epoch 186, loss: 0.0127\n","# epoch 187, loss: 0.0129\n","# epoch 188, loss: 0.0118\n","A checkpoint is saved at epoch 188!\n","# epoch 189, loss: 0.0134\n","# epoch 190, loss: 0.0122\n","# epoch 191, loss: 0.0124\n","# epoch 192, loss: 0.0126\n","# epoch 193, loss: 0.0115\n","A checkpoint is saved at epoch 193!\n","# epoch 194, loss: 0.0130\n","# epoch 195, loss: 0.0126\n","# epoch 196, loss: 0.0128\n","# epoch 197, loss: 0.0119\n","# epoch 198, loss: 0.0125\n","# epoch 199, loss: 0.0121\n"]}],"source":["import torch\n","import os\n","import numpy as np\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","num_epochs = 2\n","save_path = '/home/foamlab/nw/save/dice_correct.pt'  # Define your checkpoint save path\n","\n","# Initialize EMA\n","ema = EMAModel(\n","    parameters=nets.parameters(),\n","    power=0.75\n",")\n","\n","# Initialize optimizer\n","optimizer = torch.optim.AdamW(\n","    params=nets.parameters(),\n","    lr=1e-4, weight_decay=1e-6\n",")\n","\n","# Initialize learning rate scheduler\n","lr_scheduler = get_scheduler(\n","    name='cosine',\n","    optimizer=optimizer,\n","    num_warmup_steps=500,\n","    num_training_steps=len(dataloader) * num_epochs\n",")\n","\n","# Check for existing checkpoint\n","start_epoch = 0\n","min_loss = np.inf\n","\n","if os.path.isfile(save_path):\n","    print(\"Loading checkpoint...\")\n","    checkpoint = torch.load(save_path, map_location='cuda')\n","    nets.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n","    min_loss = checkpoint['loss']\n","    print('Checkpoint loaded, resuming from epoch', start_epoch)\n","else:\n","    print(\"No checkpoint found, starting training from scratch.\")\n","\n","# Training loop\n","for epoch_idx in range(start_epoch, num_epochs):\n","    epoch_loss = list()\n","    # Batch loop\n","    for nbatch in dataloader:\n","        # Data processing\n","        nimage = nbatch['image'][:, :obs_horizon].to(device, dtype=torch.float)\n","        nagent_pos = nbatch['agent_pos'][:, :obs_horizon].to(device)\n","        naction = nbatch['action'].to(device)\n","        B = nagent_pos.shape[0]\n","\n","        # Encoder vision features\n","        image_features = nets['vision_encoder'](nimage.flatten(end_dim=1))\n","        image_features = image_features.reshape(*nimage.shape[:2], -1)\n","\n","        # Concatenate features\n","        obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n","        obs_cond = obs_features.flatten(start_dim=1)\n","\n","        # Sample noise\n","        noise = torch.randn(naction.shape, device=device, dtype=torch.float)\n","        timesteps = torch.randint(\n","            0, noise_scheduler.config.num_train_timesteps,\n","            (B,), device=device\n","        ).long()\n","\n","        # Forward diffusion process\n","        noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n","        noisy_actions = noisy_actions.to(device, dtype=torch.float)\n","        obs_cond = obs_cond.to(device, dtype=torch.float)\n","\n","        # Predict noise residual\n","        noise_pred = noise_pred_net(noisy_actions, timesteps, global_cond=obs_cond)\n","        loss = nn.functional.mse_loss(noise_pred, noise)\n","\n","        # Optimize\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        lr_scheduler.step()\n","        ema.step(nets)\n","\n","        # Logging\n","        loss_cpu = loss.item()\n","        epoch_loss.append(loss_cpu)\n","\n","    cur_loss = np.mean(epoch_loss)\n","    print(f\"# epoch {epoch_idx}, loss: {cur_loss:.4f}\")\n","\n","    if cur_loss < min_loss:\n","        min_loss = cur_loss\n","        torch.save({\n","            'epoch': epoch_idx,\n","            'model_state_dict': nets.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': cur_loss,\n","        }, save_path)\n","        print(f\"A checkpoint is saved at epoch {epoch_idx}!\")\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.007564605657188665\n","197\n","Total epochs: 197\n","dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.010825756178688138\n","198\n","Total epochs: 198\n","dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.011889690364562515\n","139\n","Total epochs: 139\n","dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.011700081528120097\n","192\n","Total epochs: 192\n","dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.007443373260508862\n","191\n","Total epochs: 191\n","dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.010712798516117577\n","197\n","Total epochs: 197\n","dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n","0.0067173192323393\n","191\n","Total epochs: 191\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n","\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n","\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n","\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"]}],"source":["import torch\n","checkpoint = torch.load('/home/foamlab/nw/save/dice_0818_try3.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')\n","\n","checkpoint = torch.load('/home/foamlab/nw/save/tennis_0818_try3.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')\n","\n","checkpoint = torch.load('/home/foamlab/nw/save/cylinder_0818_try3.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')\n","\n","checkpoint = torch.load('/home/foamlab/nw/save/dice_nw.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')\n","\n","checkpoint = torch.load('/home/foamlab/nw/save/tennis_nw.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')\n","\n","checkpoint = torch.load('/home/foamlab/nw/save/cylinder_nw.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')\n","\n","checkpoint = torch.load('/home/foamlab/nw/save/grasp_nw.pt')\n","print(checkpoint.keys())\n","print(checkpoint['loss'])\n","print(checkpoint['epoch'])\n","total_epochs = checkpoint.get('epoch', 'Epoch info not found')\n","print(f'Total epochs: {total_epochs}')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg","timestamp":1698254384773}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
